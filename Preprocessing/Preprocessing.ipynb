{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Purpose\n",
    "\n",
    "This notebook is for preprocessing a textual dataset in preparation for LDA and other techniques. The idea is that this will replace the designer preprocess-text module.\n",
    "\n",
    "Text output from this will contain each document as a string, with tokens separated by spaces. In the event that spaces would exist in tokens (such as for n-grams), they will be replaced with underscores.\n",
    "\n",
    "## Current functionality of the notebook includes:\n",
    "\n",
    "- Lemmatization using SpaCy pretrained model\n",
    "- Defining of words to remove and save, starting from a list from spacy\n",
    "- Defining of custom regular expressions to use to match tokens to remove\n",
    "- Viewers for tokens to be removed at each step sorted by counts\n",
    "- Viewer for random comments in their processed and unprocessed forms\n",
    "- Removal of unicode characters before tokenization such as dashes and random unicode characters\n",
    "- Viewer for finding responses which contain tokens containing particular strings\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## To Do\n",
    "\n",
    "### Now\n",
    "- Parenthesis handling. There are times where we get word(word and they don't get separated. Pretty sure it's not all the time so unclear what's up.\n",
    "- Change the display_side_by_side function to not use a general replace on table. It's ruining the display of words that include table.\n",
    "- Add word search for \"Word I have\" interact\n",
    "- Add doc-id to random docs for that interact. Add unprocessed doc option to token search in the final interact.\n",
    "\n",
    "### Later\n",
    "- Language removal? Right now my general unicode string removal gets rid of chinese answers. I know there are german and spanish answers still at the very least.\n",
    "- Number removal? Currently short numbers are removed as all 1 character tokens are removed.\n",
    "- Period handling. word.other is a single token. But google.com should be a single token. How do.\n",
    "- N-gram generation. I would probably want this to occur before stop word removal. Would need to be clear how to disable it easily."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## This notebook is made with \"20210625_SES_and_SET_comments.csv\" in mind\n",
    "\n",
    "Note that if you run this notebook on the entire dataset you will be waiting for about 5 hours on a single-core machine since spacy takes its time."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "DF = pd.DataFrame"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For spacy, we will try to install a pretrained english language processing pipeline called \"en-core-web-sm\"\n",
    "\n",
    "The command to run if you don't have it is below. On Azure I had to go into the terminal, activate conda, then run the following command. Someone the notebook is not running in the conda environment on Azure or something."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#!conda activate azureml_py38\n",
    "#!python -m spacy download en_core_web_sm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import spacy\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ],
   "outputs": [],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import multiprocessing as mp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from IPython.display import display, display_html\n",
    "from ipywidgets import interact, interact_manual"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_path = \"/home/azureuser/cloudfiles/code/Data/pp-20210625_SES_and_SET_comments.csv\"\n",
    "raw_text_col = \"answer\"\n",
    "text_col = \"Preprocessed answer\"\n",
    "index_col = \"unique_comment_ID\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "raw_data = pd.read_csv(data_path)\n",
    "raw_data.set_index(index_col, inplace=True)\n",
    "raw_data.dropna(inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "display(len(raw_data))\n",
    "raw_data.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Helper Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word Counts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_word_counts(texts):\n",
    "    '''Takes an iterable (or DF) of iterables of words and returns a dict of word counts.'''\n",
    "    if type(texts) == DF:\n",
    "        texts = DF[text_col]\n",
    "    return Counter(term for doc in texts for term in doc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def word_counts_DF(texts):\n",
    "    counts = get_word_counts(texts)\n",
    "    df = DF.from_dict(counts, orient=\"index\")\n",
    "    df.index.name = \"word\"\n",
    "    df.columns = [\"count\"]\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Display"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def display_side_by_side(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html() + (\"\\xa0\" * 5) # Spaces\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def display_head_wide(df,num = 40,cols = 5):\n",
    "    num = min(num,len(df)) # Just in case num is specified to be larger than the number of entires in df\n",
    "    per_col = int(np.ceil(num/cols)) # Figure out how many to show per column\n",
    "    display_side_by_side(*[df.iloc[x: x + per_col] for x in range(0,num,per_col)]) # Display the columns. *[] used to partition the dataframe"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing to do on the raw text"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Operations done in this section should mostly replace certain characters with spaces or special characters with an equivalent character. Problematic units of text should be removed as tokens."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "raw_data[text_col] = raw_data[raw_text_col]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Replace all of the special apostrophe character with an apostrophe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "raw_data[text_col] = raw_data[text_col].str.replace(\"<U\\+0092>\",\"'\")\n",
    "# any(\n",
    "#     raw_data[text_col].str.contains(\"<U\\+0092>\")\n",
    "# )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are a variety of weird ways to separate words, some of which spacy doesn't understand. We will replace them all with spaces."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "separator_patterns = [\n",
    "    \"--\", # Double dash, likely caused by something Austin did on the import end\n",
    "    \"<U\\+0097>\", # Some random dash character\n",
    "    \"<U\\+00A0>\" # A weird space character\n",
    "    ,\"<U\\+200B>\" # Zero width space??\n",
    "    ,\"<U\\+0093>|<U\\+0094>\" # Strange quotation marks. Could be replace with actual quotes.\n",
    "    ,\"<U\\+00B7>\" # Middle dot character\n",
    "    ,\"-(\\s|$)\" # To get at word final dashes\n",
    "    ,\"(^|\\s)-\" # To get at word initial dashes\n",
    "    ,\"=\" # Some people use = as a dash sort of thing\n",
    "    ,\"<U\\+(.){4}>\" # Remove all other unicode things. Might just only do this going forward.\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for pattern in separator_patterns:\n",
    "    raw_data[text_col] = raw_data[text_col].str.replace(pattern,\" \",regex = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some code to check for a pattern in the unprocessed text."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# raw_data[\n",
    "#     raw_data[raw_text_col].str.contains(\"U\\+9017\")\n",
    "#     ].head(5)[text_col].tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing on Tokens"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining texts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "texts = raw_data.iloc[:][text_col].copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "texts.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lemmatization and tokenization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def token_and_lemma(doc):\n",
    "    return [token.lemma_.lower() for token in nlp(doc)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    mp_results = pool.map(\n",
    "        func = token_and_lemma,\n",
    "        iterable= texts)\n",
    "texts = pd.Series(data = mp_results,index= texts.index,name = text_col, copy = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "texts.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stop Word Removal"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining words to remove and save"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "words = list(word_counts_DF(texts).index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "custom_stopwords = [\n",
    "    \"-pron-\", # Spacy replaces pronouns with this. Not super meangingful and super common\n",
    "    \"firstname\",\"lastname\", # Removed for same reason as pron\n",
    "    \"na\" # I think pandas imports this as missing. Not useful anyways.\n",
    "    ]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "custom_savewords = [\n",
    "    \"enough\" # Note sure why this is stop word, I think sentiment words are still valuable for LDA\n",
    "    ,\"show\" # This feels meaningful, as in a writing class they could talk about showing versus describing\n",
    "    ,\"please\" # Seems like a strongly charged sentiment word\n",
    "    ,\"alone\" # Can matter, especially for people talking about organization of group work\n",
    "    ,\"due\" # Another word that is more relevant in school context than usual\n",
    "    ,\"see\" # Meaningful and common word\n",
    "    ,\"serious\" # Sentiment word, reasonably common\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "other_savewords_to_consider = [\"why\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating the stopword list"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "punctuation = [mark for mark in string.punctuation] # Spacy puts each piece of punctuation as its own token\n",
    "spacy_stopwords = list(nlp.Defaults.stop_words)\n",
    "# nltk_stopwords = list(nltk.corpus.stopwords.words('english')) #Don't have nltk on Azure and I don't wanna get it.\n",
    "stopwords = set(\n",
    "    custom_stopwords+\n",
    "    spacy_stopwords+\n",
    "    punctuation\n",
    "    ).intersection(words)\n",
    "stopwords = stopwords - set(custom_savewords)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Looking at words to be removed via stopwords"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "words_to_remove = word_counts_DF(texts).loc[stopwords].sort_values(by = \"count\", ascending = False)\n",
    "display_html(f\"<b>Words to remove via stopword removal {len(words_to_remove)}\",raw=True)\n",
    "@interact(n = [20,50,100,500,2000,10000])\n",
    "def look_at_stopwords(n = 50):\n",
    "    display_head_wide(df= words_to_remove,num = n,cols = 8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Actual Removal"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "texts = texts.apply(\n",
    "    lambda text: [token for token in text if token not in stopwords]\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regex Removal"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build list of new tokens to remove"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stop_regexes =[\n",
    "    \"\\s+\" # Remove all tokens consisting of only whitespace characters\n",
    "    ,\".{1}\" # Remove all tokens that are one in length. Some important 2 length words are \"ge\" and \"ok\"\n",
    "    ,\"\\.+\" # Tokens that are all periods\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "words = word_counts_DF(texts).index\n",
    "regex_stopwords = []\n",
    "for stop_regex in stop_regexes:\n",
    "    new_stopwords = words[words.str.fullmatch(stop_regex)].tolist()\n",
    "    regex_stopwords+= new_stopwords\n",
    "regex_stopwords = list(set(regex_stopwords))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check what we are getting rid of with stop regex"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "regex_words_to_remove = word_counts_DF(texts).loc[regex_stopwords].sort_values(by = \"count\",ascending=False)\n",
    "display_html(f\"<b> Words to remove via regex removal: {len(regex_words_to_remove)}\",raw = True)\n",
    "@interact(num = [20,50,200,1000,10000])\n",
    "def display_regex_to_remove(num = 50):\n",
    "    display_head_wide(\n",
    "        regex_words_to_remove,\n",
    "        num = num,\n",
    "        cols = 8\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Actual Removal"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "texts = texts.apply(\n",
    "    lambda text: [token for token in text if token not in regex_stopwords]\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Checking what words we have"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sorted_counts = word_counts_DF(texts).sort_values(by = \"count\", ascending=False)\n",
    "@interact(n = [2,10,20,50,100,500,2000])\n",
    "def look_words_by_freq(n = 20):\n",
    "    display_html(f\"Total Word Count: <b>{len(sorted_counts)}\", raw=True)\n",
    "    display_head_wide(df = sorted_counts, num = n, cols = 8) # Display most common words\n",
    "    display_head_wide(df = sorted_counts.iloc[::-1], num = n, cols = 8) # Display least common words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Checking texts again"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@interact(n = [3,20,100,500], get_new_texts = False,show_unprocessed_text = False)\n",
    "def look_at_final_texts(n = 3, show_unprocessed_text = False,get_new_texts=False,):\n",
    "    sample_texts = texts.sample(n)\n",
    "    for id in sample_texts.index:\n",
    "        if show_unprocessed_text == True:\n",
    "            display_html(\"<b>\"+raw_data.loc[id][raw_text_col],raw=True)\n",
    "        display_html(\"• \" + \" \".join(sample_texts[id]),raw=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@interact_manual(word = \"\", max_num = [20,100,1000])\n",
    "def word_search(word,max_num):\n",
    "    match_texts = texts[texts.apply(\n",
    "        lambda text: any(word in token for token in text)\n",
    "    )]\n",
    "    if len(match_texts) == 0: display(\"No matches for:\",word)\n",
    "    if len(match_texts) > max_num: match_texts = match_texts[:max_num]\n",
    "    for text in match_texts:\n",
    "        display_html(\"<br>• \"+\" \".join(text),raw=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "joined_texts = texts.apply(\" \".join)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pp_data = raw_data.copy()\n",
    "pp_data[text_col] = joined_texts\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pp_data.tail(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_path = \"pp-SES_SET_July_31.csv\"\n",
    "#pp_data.to_csv(export_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "17c92ebe4c347de728263208104da506711e9388d419f2667b4726047bdcfa3c"
  },
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit ('azureml_py38': conda)"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}