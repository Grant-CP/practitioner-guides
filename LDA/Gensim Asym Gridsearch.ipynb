{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# To do\n",
        "\n",
        "- Remove the need for filter values so there are fewer things to specify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook performs fitting of gensim LDA based on a range of parameters as described in the settings file. It expects a settings.json file in the same directory and the explorer.ipynb notebook one directory up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Click here](#display-results) to jump to the results graphs and tables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "# Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1622749792449
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import multiprocessing as mp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1622749787840
        }
      },
      "outputs": [],
      "source": [
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models import CoherenceModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from os.path import join as pjoin\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Job Settings Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:    \n",
        "    with open(\"job_config.json\") as config_file:\n",
        "        settings = json.load(config_file)\n",
        "except:\n",
        "    settings = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame([settings]).transpose() # Display the job config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_path = settings.get(\"data_filepath\",\"Your data filepath here\") # Filepath to the data\n",
        "text_col = settings.get(\"text_column\",\"Your text column name here\") # Column containing text to be analyzed\n",
        "index_col = settings.get(\"index_column\",\"Your index column name here\") # Column to use as index for dataframe\n",
        "filter_col = settings.get(\"filter_column\",\"Your filter column here\") # Column used for filtering the data\n",
        "filter_vals = settings.get(\"acceptable_values\",\"Your filter values here\") # Values accepted from said column\n",
        "pickle_model = settings.get(\"pickle_model\",False) # Whether to save the raw Gensim model. Not recommended\n",
        "delete_bad_runs = settings.get(\"delete_bad_runs\",True) # Whether to delete the files associated with most runs. Important for saving disk space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "# Import and Vectorize Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessed_SES = pd.read_csv(data_path,\n",
        "            usecols = [index_col,filter_col,text_col],\n",
        "            index_col = False)\n",
        "# Set the index manually as I was having some trouble with specifying it in the import statement.\n",
        "preprocessed_SES.set_index(index_col,inplace = True)\n",
        "len(preprocessed_SES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove any missing rows. This is necessary since some people write \"nan\". Shouldn't be a problem with new preprocessing.\n",
        "preprocessed_SES.dropna(inplace = True)\n",
        "len(preprocessed_SES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Filter rows based on the filter_col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Select responses from columns specified in the settings file\n",
        "preprocessed_SES = preprocessed_SES[preprocessed_SES[filter_col].isin(filter_vals)]\n",
        "len(preprocessed_SES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessed_SES.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenize the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Split each document into a list of words (a 'text' as gensim calls it)\n",
        "tokenized_SES = preprocessed_SES[text_col].apply(str.split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the word list for the texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Create Word to ID pairing for use with gensim models\n",
        "SES_dictionary = Dictionary(tokenized_SES)\n",
        "#Remove super common words. Not sure if necessary or helpful.\n",
        "SES_dictionary.filter_extremes(no_above = .5)\n",
        "# Remove all very short words\n",
        "short_words = [word for word in SES_dictionary.token2id.keys() if len(word) < 2]\n",
        "short_ids = [SES_dictionary.token2id[word] for word in short_words]\n",
        "# Remove any specified stop words\n",
        "stop_words = settings.get(\"stop_words\",[])\n",
        "stop_ids = [SES_dictionary.token2id.get(word,None) for word in stop_words]\n",
        "stop_ids = list(filter(None,stop_ids))\n",
        "SES_dictionary.filter_tokens(\n",
        "    bad_ids = short_ids + stop_ids\n",
        ")\n",
        "# Create corpus.\n",
        "SES_corpus = [SES_dictionary.doc2bow(x) for x in tokenized_SES]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Functions for scoring and extracting components from the fitted LDA Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def score_lda(lda,corpus):\n",
        "    '''This function takes a fitted gensim LdaModel and returns a dictonary of scores'''\n",
        "    #Calculate perplexity from gensim method as per gensim documentation and source code.\n",
        "    perplexity_score = 2**(-lda.log_perplexity(corpus))\n",
        "    # Fit and score coherence model on the topics.\n",
        "    c_model = CoherenceModel(model = lda,\n",
        "                            texts = tokenized_SES,\n",
        "                            dictionary = SES_dictionary,\n",
        "                            coherence = 'c_v',\n",
        "                            processes = 1)\n",
        "    cv_score = c_model.get_coherence()\n",
        "    return {\n",
        "        \"cv_score\": cv_score,\n",
        "        \"perplexity\": perplexity_score\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_term_topic_matrix(lda):\n",
        "    '''Takes a fitted gensim LDA model and returns a dataframe with words on the index and topics as columns'''\n",
        "    #First grab the matrix of word probabilities\n",
        "    term_topic_matrix = pd.DataFrame(lda.get_topics()).transpose()\n",
        "    #Replace index with word. Pretty sure the index matches correctly.\n",
        "    term_topic_matrix.rename(\n",
        "        index = SES_dictionary.id2token,\n",
        "        columns = str, # Change columns to have string names\n",
        "        inplace = True\n",
        "        )\n",
        "    return term_topic_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_doc_topic_matrix(lda):\n",
        "    '''Takes a fitten gensim LDA model and returns a dataframe of the estimated document distributions'''\n",
        "    # A real issue is that the results of get_document topics seem to be variable. For instance it can assign .36 to topic 1 or to topic 5\n",
        "    # Use the fitted model on the dataset\n",
        "    document_topic_matrix = pd.DataFrame(\n",
        "        [{doc_tuple[0]:doc_tuple[1] for doc_tuple in doc_tuple_list} for doc_tuple_list in lda[SES_corpus]])\n",
        "    # Fill in missing values.\n",
        "    document_topic_matrix.fillna(0,inplace = True)\n",
        "    # Reorder columns to be nice\n",
        "    document_topic_matrix = document_topic_matrix.reindex(sorted(document_topic_matrix.columns), axis=1)\n",
        "    # Change columns to have string names\n",
        "    document_topic_matrix.rename(columns = str, inplace = True)\n",
        "    # Introduce the actual document index\n",
        "    document_topic_matrix.index = preprocessed_SES.index\n",
        "    return document_topic_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Do the gridsearch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure Runs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_topics_list = settings.get(\"num_topics\",[3,5,10]) # Load in hyperparameter space\n",
        "runs = []\n",
        "run_id = 1\n",
        "for n in num_topics_list:\n",
        "    run = {\n",
        "        \"run_id\": run_id,\n",
        "        \"num_topics\": n,\n",
        "        \"run_name\": f\"{n}-top-run-{run_id}\" # This should be a nice directory name\n",
        "    }\n",
        "    run_id += 1\n",
        "    runs.append(run) # add run to list of runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_docs = len(preprocessed_SES)\n",
        "passes = int(np.ceil(100000/num_docs)) # Make sure the fitting looks at at least 100,000 documents.\n",
        "def execute_run(run):\n",
        "    run_results = run.copy() # Make a copy of run to add onto\n",
        "    num_topics = run[\"num_topics\"] # Number of topics to use for fitting\n",
        "    run_folder = run[\"run_name\"] # Name to use for folder\n",
        "    model_name = settings.get(\"job_name\",\"lda\") # Name to use for saved model files\n",
        "    random_state = settings.get(\"random_state\",333)\n",
        "    iterations = settings.get(\"iterations\",50)\n",
        "    # Fit model\n",
        "    lda = LdaModel(\n",
        "        id2word = SES_dictionary,\n",
        "        iterations = iterations, # Max number of iterations of model?\n",
        "        passes = passes, # Number of times to go through the texts\n",
        "        num_topics = num_topics,\n",
        "        alpha = 'auto', # Learn possibly asymmetric alpha\n",
        "        random_state = random_state\n",
        "        )\n",
        "    lda.update(SES_corpus)\n",
        "    # Score model\n",
        "    scores = score_lda(lda,corpus = SES_corpus)\n",
        "    # Add scores to run results\n",
        "    run_results.update(scores)\n",
        "    # Create run folder\n",
        "    os.makedirs(run_folder,exist_ok = True)\n",
        "    # Save fitted LDA model in run folder if that option was selected\n",
        "    if pickle_model:\n",
        "        lda.save(\n",
        "            pjoin(run_folder,model_name)\n",
        "            )\n",
        "    # Put explorer notebook in run folder\n",
        "    try:\n",
        "        shutil.copyfile(\n",
        "            src = \"../Explorer.ipynb\",\n",
        "            dst = pjoin(run_folder,\"Explorer.ipynb\")\n",
        "        )\n",
        "    except:\n",
        "        shutil.copyfile(\n",
        "            src = \"Interactive-LDA-Explorer.ipynb\",\n",
        "            dst = pjoin(run_folder,\"Explorer.ipynb\")\n",
        "        )\n",
        "    # Copy the settings file to the run folder\n",
        "    shutil.copyfile(\n",
        "        src = \"job_config.json\",\n",
        "        dst = pjoin(run_folder,\"settings.json\")\n",
        "    )\n",
        "    # Save the document topic matrix and term topic matrices to the run folder\n",
        "    get_doc_topic_matrix(lda).to_parquet(\n",
        "        pjoin(run_folder,\"doc-topic.parquet\")\n",
        "    )\n",
        "    get_term_topic_matrix(lda).to_parquet(\n",
        "        pjoin(run_folder,\"term-topic.parquet\")\n",
        "    )\n",
        "    return run_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute in Parallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cpu_count = mp.cpu_count()\n",
        "print(f\"CPUs to be used: {cpu_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with mp.Pool(cpu_count) as pool:\n",
        "    results = pool.map(execute_run,runs)\n",
        "    grid = pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Delete runs results for bad runs (if option is selected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def delete_run_results(run):\n",
        "    '''Deletes the run folder containing the results from the specified run'''\n",
        "    run_folder = run[\"run_name\"] # Get the folder name from the run dict\n",
        "    shutil.rmtree(run_folder)\n",
        "    print(f\"Deleted run in {run_folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Determine which runs to save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find ranges of topic_num based on quantiles, then select the highest cv_score element from each.\n",
        "quantile_borders = grid[\"num_topics\"].quantile(\n",
        "    q = [0, 1/5, 2/5, 3/5, 1],\n",
        "    interpolation='nearest'\n",
        ").tolist()\n",
        "print(quantile_borders)\n",
        "ranges_of_interest = [range(quantile_borders[i],quantile_borders[i+1]) for i in range(len(quantile_borders)-1)]\n",
        "idxs_to_save = []\n",
        "for rang in ranges_of_interest:\n",
        "    if len(rang) > 0: # In case an interquartile range is empty due to multiple quantiles being the same.\n",
        "        idx_to_save = grid[grid[\"num_topics\"].isin(rang)][\"cv_score\"].idxmax()\n",
        "        idxs_to_save.append(idx_to_save)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Delete the other runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if delete_bad_runs:\n",
        "    grid.drop(idxs_to_save).apply(\n",
        "        delete_run_results,\n",
        "        axis=1\n",
        "    );"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Display results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Job results for:\\n\",\"\\t\",settings.get(\"long_name\"))\n",
        "print(f\"Passes: {passes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid[[\"run_id\",\"num_topics\",\"cv_score\",\"perplexity\"]].sort_values(\n",
        "    by = \"cv_score\",\n",
        "    ascending = False).style.hide_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I suspect that something is wrong give that perplexity seems to be increasing with number of topics. This happened in the SKLearn implementation of LDA also, and we aren't using perplexity for our model choice anyway."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(\"Lower perplexity is better\")\n",
        "grid.plot(x='num_topics',\n",
        "          y='perplexity')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "C_V Score is giving some local maxima, which is nice. The idea behind c_v score is that it grades topics based on how similar the top words are. Top words are defined by p(word|topic) and similarlity is defined by a prefit coherence model drawn from wikipedia text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Higher coherence score is better\")\n",
        "grid.plot(x='num_topics',\n",
        "          y='cv_score')"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "17c92ebe4c347de728263208104da506711e9388d419f2667b4726047bdcfa3c"
    },
    "kernelspec": {
      "display_name": "Python 3.8.1 64-bit ('azureml_py38': conda)",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    },
    "orig_nbformat": 3
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
